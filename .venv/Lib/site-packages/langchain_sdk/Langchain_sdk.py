import logging
import json
import re
from typing import Any, Dict, List, Iterator, Optional
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from .client import IgptAPIClient
from langchain_core.embeddings import Embeddings


class LangChainCustomEmbeddings(Embeddings):

    embedding_model_name: str
    client_id: str = None
    client_secret: str = None
    proxy_url: Optional[str] = None
    auth_url: Optional[str] = None
    api_url_embed:  Optional[str] = None

    api_client = IgptAPIClient(client_id, client_secret, proxy_url, auth_url, api_url_embed)

    def __init__(self, embedding_model_name: str, client_id: str, client_secret: str, proxy_url: Optional[str] = None, auth_url: Optional[str] = None,  
                 api_url_embed: Optional[str] = None, **kwargs):

        if not client_id:
            raise ValueError("client_id is a required parameter")
        if not client_secret:
            raise ValueError("client_secret is a required parameter")
        if not embedding_model_name:
            raise ValueError("embedding_model_name is a required parameter")
        logging.info('LangChainCustomEmbeddings instance created.')
        self.embedding_model_name=embedding_model_name
        self.client = IgptAPIClient(client_id, client_secret, proxy_url, auth_url, api_url_embed) 

    def check_texts(self, texts: List[str]):
        if not all(text.strip() for text in texts):
            raise ValueError("All texts must be non-empty and cannot be just whitespace")
        
    def embed_documents(
        self, texts: List[str], chunk_size: Optional[int] = 0
    ) -> List[List[float]]:
        
        if not isinstance(texts, list):
            raise ValueError("Input should be a list of strings")
        if not texts:
            raise ValueError("texts can not be empty")
        self.check_texts(texts) 

        try:
            json_data ={
                    "options": {            
                    "model": self.embedding_model_name
                        },            
                    "inputTexts":texts        
            }
            response = self.client.process_request_embed(json_data)
            logging.info('Embeddings retrieved successfully.')
            if 'embeddings' not in response.json().keys():
                return response.json()
            else :
                return response.json()['embeddings']
        except (ValueError, ConnectionError) as e:
            logging.error(f"Error occurred while getting embeddings: {e}")
            return None 
    
    def embed_query(self, text: str) -> List[float]:
        if not isinstance(text, str):
             raise ValueError("Input should be string")
        
        return self.embed_documents([text])[0]

class LangChainCustom(LLM):
    client_id: str = None
    client_secret: str = None
    model: str
    temperature: float
    system_prompt: str
    max_Tokens: Optional[int] = None
    top_P: Optional[float] = None
    frequency_Penalty: Optional[float] = None
    presence_Penalty: Optional[float] = None
    stop: Optional[str] = None
    allowModelFallback: Optional[bool] = None
    auth_url: Optional[str] = None
    proxy_url: Optional[str] = None
    api_url: Optional[str] = None
    api_url_stream: Optional[str] = None
    chat_conversation:Optional[bool] = None
    conversation_history: Optional[list] = None
    client: IgptAPIClient = IgptAPIClient(client_id, client_secret, auth_url, proxy_url, api_url, api_url_stream)
    
    def __init__(self, client_id: str, client_secret: str, auth_url: Optional[str] = None, proxy_url: Optional[str] = None,  
                 api_url: Optional[str] = None, api_url_stream: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if not client_id:
            raise ValueError("client_id is a required parameter")
        if not client_secret:
            raise ValueError("client_secret is a required parameter")
        logging.info('LangChainCustom instance created.')
        self.client = IgptAPIClient(client_id, client_secret, auth_url, proxy_url, api_url, api_url_stream)

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        
        """ Run the LLM on the given input. """
        try:
            org_params = {
                        "model": self.model,
                        "temperature": self.temperature,
                        "max_Tokens": self.max_Tokens,
                        "top_P": self.top_P,
                        "frequency_Penalty": self.frequency_Penalty,
                        "presence_Penalty": self.presence_Penalty,
                        "stop": self.stop,
                        "allowModelFallback": self.allowModelFallback 
                        }
            filt_params = {k: v for k, v in org_params.items() if v is not None}          # removing parameters having none values
            conversation =  self.conversation_history
            if(conversation is not None):
                conversation.append({
                            "role": "user",
                            "content": prompt
                        })
            else:
                conversation={
                            "role": "user",
                            "content": prompt
                        }

            json_data ={
                "options":
                    filt_params
                    ,
                "conversation": conversation
            }
            response = self.client.process_request(json_data)
            logging.info('Response retrieved successfully.')
            if self.chat_conversation:
                return str(response.content)
            response = json.loads(response.content.decode('utf-8'))['currentResponse']
            return str(response)
        except (ValueError, ConnectionError) as e:
            logging.error(f"Error occurred while getting response from prompt: {e}")
            return None 
    
    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        
        """ Stream the LLM on the given input."""
        try:
            org_params = {
                        "model": self.model,
                        "temperature": self.temperature,
                        "max_Tokens": self.max_Tokens,
                        "top_P": self.top_P,
                        "frequency_Penalty": self.frequency_Penalty,
                        "presence_Penalty": self.presence_Penalty,
                        "stop": self.stop,
                        "allowModelFallback": self.allowModelFallback
                        }
            filt_params = {k: v for k, v in org_params.items() if v is not None}          # removing parameters having none values
            json_data ={
                "options":
                    filt_params
                    ,
                "conversation": [
                    {
                        "role": "system",
                        "content": self.system_prompt
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }
            for response in self.client.process_request_stream(json_data):
                cleaned_string = re.sub(r"^b[\'\"](.*?)[\'\"]$", r'\1', str(response)).encode('utf-8').decode('unicode-escape')
                split_string = cleaned_string.split("\r\r")[:-1]
                for line in split_string:
                    json_str = line.replace('data: ', '')
                    json_obj = json.loads(json_str)
                    chunk = GenerationChunk(text=json_obj['currentResponse'])
                    if run_manager:
                            run_manager.on_llm_new_token(chunk.text, chunk=chunk)
                    yield chunk
        except (ValueError, ConnectionError) as e:
            logging.error(f"Error occurred while streaming response from prompt: {e}")
            return None 
   

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters."""
        return {
            "model_name": self.model,
        }

    @property
    def _llm_type(self) -> str:
        """Get the type of language model used by this chat model. Used for logging purposes only."""
        return "langchain llm"
    
